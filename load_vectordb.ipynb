{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- I need to make a function that wraps the creation of a langchain chroma retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your working directory\n",
    "working_dir = \"/Users/pietromascheroni/open-modular-rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from torch import cuda\n",
    "from typing import Callable\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import chromadb\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pietromascheroni/open-modular-rag/env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/pietromascheroni/open-modular-rag/env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embed_model_id = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size': 32},\n",
    "    cache_folder=working_dir + '/emb_model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromaDB setup to initilize collection including indeces of all documents\n",
    "# (in case of errors, perform pip uninstall chromadb and pip install chromadb)\n",
    "chroma_client = chromadb.PersistentClient(path=working_dir + \"/vectordb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide a name to setup and reference the vector index\n",
    "collection_name = \"more_agents_paper_self_rag\"\n",
    "# initialize the vector index with the respective similarity search metric\n",
    "vectorstore = chroma_client.get_or_create_collection(collection_name, metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 139 chunks in the vector store\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {vectorstore.count()} chunks in the vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 139 in the langchain-formatted collection\n"
     ]
    }
   ],
   "source": [
    "# Load the vectordb as a langchain object\n",
    "langchain_chroma = Chroma(\n",
    "    client=chroma_client,\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=embedding_model,\n",
    ")\n",
    "\n",
    "print(\"There are\", langchain_chroma._collection.count(), \"in the langchain-formatted collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 0 : across a wide range of tasks. Surpris- ingly, a brute-force ensemble of smaller LLMs can achieve comparable or superior performance to larger LLMs, with a nutshell shown in Figure 1, which will be further expanded in later sections. Moreover, by combining our method with other existing methods, we find the performance can be further improved. By comparing with the performance of complicated\n",
      "chunk 1 : task designed to isolate each one. Consider the task detailed below: To start the analysis, we select two datasets with increasing difficulty, i.e., GSM8K and MATH, to calculate the rela- tive performance gain. The relative performance gain η is given by: η = Pm−Ps where Pm and Ps are the perfor- mances (accuracy) with our method and a single LLM query, respectively. The results are shown in\n",
      "chunk 2 : compared with the result of GPT-4 with K = 32, the hierarchical method improves performance from 35% to 47%, suggesting the deployment of different LLMs at the corresponding level of problem- solving can improve the performance in a cost-effective manner. Furthermore, we observe that the performance gains are influenced by the difficulty of the task. To explore this correlation, we isolate and\n",
      "chunk 3 : when utilizing different LLMs. From these curves, we demonstrate that our method has the following properties: Generalizability. By using our method standalone, the performance can be generally enhanced by increasing the ensemble size. Compatibility. Our method can generally enhance other methods by increasing the ensemble size. 12\n",
      "chunk 4 : on these properties, we also develop ways to boost the effectiveness of “More Agents”. 7. Conclusions and Future Work In this paper, we report that , i.e., simply adding more instantiated LLM agents is what you need to obtain a better LLM performance in processing com- plex tasks, without bothering complicated methods, such as CoT pipelines, multi-agent collaboration frameworks, etc. Considering\n"
     ]
    }
   ],
   "source": [
    "# Query the database\n",
    "query = \"improve performance of LLMs\"\n",
    "docs = langchain_chroma.similarity_search(query, k=5)\n",
    "\n",
    "# print results\n",
    "for i, doc in enumerate(docs):\n",
    "    print(\"chunk\", i, \":\", doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Wu et al., 2023). In these works, multiple LLM agents are used to improve the performance of LLMs. For instance, LLM-Debate (Du et al., 2023) employs multiple LLM agents in a debate form. The reasoning performance is improved by creating a frame- work that allows more than one agent to “debate” the final answer of arithmetic tasks. They show performance im- provements compared to using one single', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '1', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = langchain_chroma.as_retriever(\n",
    "    search_kwargs={'k': 1, 'filter': {'Page': '1'}}\n",
    "    )\n",
    "\n",
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hnsw:space': 'cosine'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_chroma._collection.metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
