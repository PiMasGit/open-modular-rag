{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-rag\n",
    "I am following the notebook [here](https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag_local.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![schematic of self-rag](graph_self_rag.png \"Schematic of Self-Rag from [here](https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag_local.ipynb).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your working directory\n",
    "working_dir = \"/Users/pietromascheroni/open-modular-rag\"\n",
    "# specify the number of retrieved chunks\n",
    "K = 5\n",
    "# specify the number of maximum query re-generations\n",
    "N = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from torch import cuda\n",
    "from typing import Callable\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import chromadb\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pietromascheroni/open-modular-rag/env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/pietromascheroni/open-modular-rag/env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embed_model_id = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size': 32},\n",
    "    cache_folder=working_dir + '/emb_model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromaDB setup to initilize collection including indeces of all documents\n",
    "# (in case of errors, perform pip uninstall chromadb and pip install chromadb)\n",
    "chroma_client = chromadb.PersistentClient(path=working_dir + \"/vectordb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide a name to setup and reference the vector index\n",
    "collection_name = \"more_agents_paper_self_rag\"\n",
    "# initialize the vector index with the respective similarity search metric\n",
    "vectorstore = chroma_client.get_or_create_collection(collection_name, metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 139 chunks in the vector store\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {vectorstore.count()} chunks in the vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['task designed to isolate each one. Consider the task detailed below: To start the analysis, we select two datasets with increasing difficulty, i.e., GSM8K and MATH, to calculate the rela- tive performance gain. The relative performance gain η is given by: η = Pm−Ps where Pm and Ps are the perfor- mances (accuracy) with our method and a single LLM query, respectively. The results are shown in',\n",
       "  'based on Table 2. Task Llama2-13B Llama2-70B GPT-3.5-Turbo GSM8K (easy) MATH (hard) 69 200 37 120 16 34 It is noteworthy that the relative performance gain is more substantial with increasing task difficulty. Specifically, we observe that within the same task, the smaller model, Llama2-13B, gains ranging from 28%-200%, but only 8%- 16% over GPT-3.5-Turbo. Moreover, the more challenging task MATH',\n",
       "  'for a given task. Nodes represent steps, while dashed lines indicate alternative potential steps. The depth of nodes represents the number of steps, and the color intensity represents the level of inherent difficulty. Table 5. The relative performance gain (%) becomes more signif- icant when the relative difficulty between the LLM and the task increases. It is calculated based on Table 2. Task',\n",
       "  'across a wide range of tasks. Surpris- ingly, a brute-force ensemble of smaller LLMs can achieve comparable or superior performance to larger LLMs, with a nutshell shown in Figure 1, which will be further expanded in later sections. Moreover, by combining our method with other existing methods, we find the performance can be further improved. By comparing with the performance of complicated',\n",
       "  'when utilizing different LLMs. From these curves, we demonstrate that our method has the following properties: Generalizability. By using our method standalone, the performance can be generally enhanced by increasing the ensemble size. Compatibility. Our method can generally enhance other methods by increasing the ensemble size. 12']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_query = embedding_model.embed_documents(['LLM performance'])\n",
    "output = vectorstore.query(query_embeddings=emb_query, n_results=5, include=['distances','documents'])\n",
    "output['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 139 in the langchain-formatted collection\n"
     ]
    }
   ],
   "source": [
    "# Load the vectordb as a langchain object\n",
    "langchain_chroma = Chroma(\n",
    "    client=chroma_client,\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=embedding_model,\n",
    ")\n",
    "\n",
    "print(\"There are\", langchain_chroma._collection.count(), \"in the langchain-formatted collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='task designed to isolate each one. Consider the task detailed below: To start the analysis, we select two datasets with increasing difficulty, i.e., GSM8K and MATH, to calculate the rela- tive performance gain. The relative performance gain η is given by: η = Pm−Ps where Pm and Ps are the perfor- mances (accuracy) with our method and a single LLM query, respectively. The results are shown in', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '6', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
       " Document(page_content='based on Table 2. Task Llama2-13B Llama2-70B GPT-3.5-Turbo GSM8K (easy) MATH (hard) 69 200 37 120 16 34 It is noteworthy that the relative performance gain is more substantial with increasing task difficulty. Specifically, we observe that within the same task, the smaller model, Llama2-13B, gains ranging from 28%-200%, but only 8%- 16% over GPT-3.5-Turbo. Moreover, the more challenging task MATH', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '6', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
       " Document(page_content='for a given task. Nodes represent steps, while dashed lines indicate alternative potential steps. The depth of nodes represents the number of steps, and the color intensity represents the level of inherent difficulty. Table 5. The relative performance gain (%) becomes more signif- icant when the relative difficulty between the LLM and the task increases. It is calculated based on Table 2. Task', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '6', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
       " Document(page_content='across a wide range of tasks. Surpris- ingly, a brute-force ensemble of smaller LLMs can achieve comparable or superior performance to larger LLMs, with a nutshell shown in Figure 1, which will be further expanded in later sections. Moreover, by combining our method with other existing methods, we find the performance can be further improved. By comparing with the performance of complicated', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '2', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
       " Document(page_content='when utilizing different LLMs. From these curves, we demonstrate that our method has the following properties: Generalizability. By using our method standalone, the performance can be generally enhanced by increasing the ensemble size. Compatibility. Our method can generally enhance other methods by increasing the ensemble size. 12', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '12', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = langchain_chroma.as_retriever(\n",
    "    search_kwargs={'k': K,\n",
    "                   #'filter': {'Page': '1'}\n",
    "                   }\n",
    "    )\n",
    "\n",
    "test_query = \"LLM performance\"\n",
    "retriever.invoke(test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environmental variables\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# set up the LLM\n",
    "llm = ChatGroq(temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n\n",
    "    If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"more agents improve performance\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # alternative initialization: this enforces directly the json format\n",
    "# structured_llm = llm.with_structured_output(method=\"json_mode\", include_raw=False)\n",
    "\n",
    "# retrieval_grader_experimental = prompt | structured_llm\n",
    "# print(retrieval_grader_experimental.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the documents suggest that using multiple agents can improve performance in certain tasks, as seen in the accuracy increase with ensemble size. Simply adding more instantiated LLM agents is reported to obtain a better LLM performance in processing complex tasks. However, it's important to note that input from multiple agents can sometimes disrupt coherence, leading to performance degradation.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To visualize the prompt template of rag\n",
    "# print(prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucination grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an answer is grounded in / supported by a set of facts. \\n \n",
    "    Here are the facts:\n",
    "    \\n ------- \\n\n",
    "    {documents} \n",
    "    \\n ------- \\n\n",
    "    Here is the answer: {generation}\n",
    "    Give a binary score 'yes' or 'no' to indicate whether the answer is grounded in / supported by a set of facts. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an answer is useful to resolve a question. \\n \n",
    "    Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {generation} \n",
    "    \\n ------- \\n\n",
    "    Here is the question: {question}\n",
    "    Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question rewriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Does increasing the number of agents lead to improved performance?\"'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "re_write_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a question re-writer that converts an input question to a better version that is optimized for vectorstore retrieval. Look at the initial question and formulate an improved question. \\n\n",
    "     Here is the initial question: \\n\\n {question}. Improved question with no preamble: \\n \"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the graph using Langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "        numret: number of re-generations of the query\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "    numret: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        try:\n",
    "            score = retrieval_grader.invoke( # added handling of exception (since we work with json output)\n",
    "                {\"question\": question, \"document\": d.page_content}\n",
    "            )\n",
    "            grade = score[\"score\"]\n",
    "        except Exception as err:\n",
    "            print(\"The error is:\", err)\n",
    "            grade == \"N/A\"\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        elif grade == \"no\":\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "        else:\n",
    "            print(\"An error occured in grade_documents.\")\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"generation\": \"N/A\"}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    numret = state[\"numret\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    numret += 1\n",
    "    return {\"documents\": documents, \"question\": better_question, \"numret\": numret, \"generation\": \"N/A\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered and no one was returned as relevant\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO THE QUESTION, TRY TO TRANSFORM QUERY---\"\n",
    "        )\n",
    "        print(\"---ASSESS NUMBER OF REGENERATIONS---\")\n",
    "        numret = state[\"numret\"]\n",
    "\n",
    "        if numret <= N:\n",
    "            print(f\"DECISION: the number of regeneration is still low. We can continue.\")\n",
    "            return \"transform_query\"\n",
    "        else:\n",
    "            print(f\"DECISION: We overcome the limit of query regeneration. Exiting the loop.\")\n",
    "            return \"rate_limited\"\n",
    "\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    try:\n",
    "        score = hallucination_grader.invoke(\n",
    "            {\"documents\": documents, \"generation\": generation}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "    except Exception as err:\n",
    "        print(\"The error is:\", err)\n",
    "        grade == \"N/A\"\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        try:\n",
    "            score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "            grade = score[\"score\"]\n",
    "        except Exception as err:\n",
    "            print(\"The error is:\", err)\n",
    "            grade == \"N/A\"\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        elif grade == \"no\":\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            print(\"---ASSESS NUMBER OF REGENERATIONS---\")\n",
    "            numret = state[\"numret\"]\n",
    "\n",
    "            if numret <= N:\n",
    "                print(f\"DECISION: the number of regeneration is still low. We can continue.\")\n",
    "                return \"not useful\"\n",
    "            else:\n",
    "                print(f\"DECISION: We overcome the limit of query regeneration. Exiting the loop.\")\n",
    "                return \"rate_limited\"\n",
    "\n",
    "        else:\n",
    "            print(\"An error occured in the answer_grader inside the hallucination grader\")\n",
    "\n",
    "    elif grade == \"no\":\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "    else:\n",
    "        print(\"An error occured in the hallucination grader\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve) \n",
    "workflow.add_node(\"grade_documents\", grade_documents)  \n",
    "workflow.add_node(\"generate\", generate)  \n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "        \"rate_limited\": END\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "        \"rate_limited\": END\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the self rag over the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "{ 'documents': [ Document(page_content='we can no- tice the effects of putting multiple agents together, to some extent, can lead to a performance improvement in certain problems. For example, in Table 10 of Section 3.3 of LLM- Equal contribution 1Tencent Inc. Correspondence to: Deheng Ye <dericye@tencent.com>. Figure 1. The accuracy increases with ensemble size across Llama2-13B, Llama2-70B and GPT-3.5-Turbo in GSM8K. When the', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '1', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
      "                 Document(page_content='than the relationship between the number of agents and performance. We also select representative methods (Du et al., 2023; Shinn et al., 2023) to combine with our method, achieving further enhancements. 3. Method Tasks Our method is evaluated on the following task: In this section, we introduce our method which is imple- mented through a two-phase process: sampling and voting. The overview of', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '3', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
      "                 Document(page_content='of experiments, we adjust these dimensions and observe their effects independently. We observe and summarize a few properties, based on which, we further develop optimization strategies that can intrigue the power of “More Agents”. Our contributions are summarized as follows: We present the first systematic study on the scaling property of raw agents instantiated by LLMs. We find that the', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '2', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
      "                 Document(page_content='in- put from multiple agents, disrupt the coherence of the code logic, leading to the observed performance degradation. All accuracy curves are provided in the Appendix B. 2Llama2-Chat 4', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '4', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
      "                 Document(page_content='generate answers. Sub- sequently, majority voting is applied to these answers to determine the final answer. Specifically, an LLM agent refers to a single LLM or a multiple LLM-Agents collaboration framework. V (si) ← V (si) + sim(si, sj) end if end for 4. Experimental Setup We separate the experimental setup (this section) with eval- uations (next section), to introduce the coverage of scenar-', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '3', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'})],\n",
      "  'question': 'Explain how performance changes with the number of agents.'}\n",
      "\n",
      "---------\n",
      "\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "{ 'documents': [ Document(page_content='we can no- tice the effects of putting multiple agents together, to some extent, can lead to a performance improvement in certain problems. For example, in Table 10 of Section 3.3 of LLM- Equal contribution 1Tencent Inc. Correspondence to: Deheng Ye <dericye@tencent.com>. Figure 1. The accuracy increases with ensemble size across Llama2-13B, Llama2-70B and GPT-3.5-Turbo in GSM8K. When the', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '1', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
      "                 Document(page_content='of experiments, we adjust these dimensions and observe their effects independently. We observe and summarize a few properties, based on which, we further develop optimization strategies that can intrigue the power of “More Agents”. Our contributions are summarized as follows: We present the first systematic study on the scaling property of raw agents instantiated by LLMs. We find that the', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '2', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'})],\n",
      "  'generation': 'N/A',\n",
      "  'question': 'Explain how performance changes with the number of agents.'}\n",
      "\n",
      "---------\n",
      "\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "{ 'documents': [ Document(page_content='we can no- tice the effects of putting multiple agents together, to some extent, can lead to a performance improvement in certain problems. For example, in Table 10 of Section 3.3 of LLM- Equal contribution 1Tencent Inc. Correspondence to: Deheng Ye <dericye@tencent.com>. Figure 1. The accuracy increases with ensemble size across Llama2-13B, Llama2-70B and GPT-3.5-Turbo in GSM8K. When the', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '1', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
      "                 Document(page_content='of experiments, we adjust these dimensions and observe their effects independently. We observe and summarize a few properties, based on which, we further develop optimization strategies that can intrigue the power of “More Agents”. Our contributions are summarized as follows: We present the first systematic study on the scaling property of raw agents instantiated by LLMs. We find that the', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '2', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'})],\n",
      "  'generation': 'The performance of certain models can improve with an '\n",
      "                'increase in the number of agents. This is because multiple '\n",
      "                'agents working together can lead to performance enhancements '\n",
      "                'in specific problems. For instance, accuracy increases with '\n",
      "                'ensemble size for models like Llama2-13B, Llama2-70B, and '\n",
      "                'GPT-3.5-Turbo in the GSM8K task. However, the specifics of '\n",
      "                'these improvements can vary and are subject to further study '\n",
      "                'and optimization.',\n",
      "  'question': 'Explain how performance changes with the number of agents.'}\n",
      "\n",
      "---------\n",
      "\n",
      "('The performance of certain models can improve with an increase in the number '\n",
      " 'of agents. This is because multiple agents working together can lead to '\n",
      " 'performance enhancements in specific problems. For instance, accuracy '\n",
      " 'increases with ensemble size for models like Llama2-13B, Llama2-70B, and '\n",
      " 'GPT-3.5-Turbo in the GSM8K task. However, the specifics of these '\n",
      " 'improvements can vary and are subject to further study and optimization.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"question\": \"Explain how performance changes with the number of agents.\", \"numret\": 1}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        pprint(value, indent=2, width=80, depth=None)\n",
    "    print(\"\\n---------\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "{ 'documents': [ Document(page_content='al., 2023). Liu et al. (2023) enables agents to interact for multiple rounds in a dynamic architecture. Li et al. (2023a); Hong et al. (2023); Wu et al. (2023); Chen et al. (2023c;a) offer several multi-agent frameworks that enable the development of LLM applications or enhance task-solving capabilities. However, these methods primarily 2', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '2', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
      "                 Document(page_content='for llm agents: A social psychology view. arXiv preprint arXiv:2310.02124, 2023. Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., Liu, P., Nie, J.-Y., and Wen, J.-R. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. 11', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '11', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
      "                 Document(page_content='However, since the scaling property of “raw” agents is not the focus of these works, the scenarios/tasks and experiments considered are limited. So far, there lacks a dedicated in-depth study on this phenomenon. Hence, a natural question arises: Does this phenomenon generally exist? To answer the research question above, we conduct the first comprehensive study on the scaling property of LLM', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '1', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
      "                 Document(page_content='els: A task-solving agent through multi-persona self- collaboration. CoRR, abs/2307.05300, 2023c. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain-of- thought prompting elicits reasoning in large language models. In NeurIPS, 2022. Wu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C.', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '11', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
      "                 Document(page_content='to using one single agent. Similarly, CoT-SC (Wang et al., 2023b) generates multiple thought chains and picks the most self-consistent one as the final an- swer. The reasoning performance is improved by involving more thought chains compared to chain-of-thought (CoT) (Wei et al., 2022) which employs a single thought chain. In- cidentally, from the data analysis of these works, we can no- tice the', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '1', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'})],\n",
      "  'question': 'How does memory work for agents?'}\n",
      "\n",
      "---------\n",
      "\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO THE QUESTION, TRY TO TRANSFORM QUERY---\n",
      "---ASSESS NUMBER OF REGENERATIONS---\n",
      "DECISION: the number of regeneration is still low. We can continue.\n",
      "\"Node 'grade_documents':\"\n",
      "{ 'documents': [],\n",
      "  'generation': 'N/A',\n",
      "  'question': 'How does memory work for agents?'}\n",
      "\n",
      "---------\n",
      "\n",
      "---TRANSFORM QUERY---\n",
      "\"Node 'transform_query':\"\n",
      "{ 'documents': [],\n",
      "  'generation': 'N/A',\n",
      "  'numret': 2,\n",
      "  'question': '\"Mechanism of memory function in agents\"'}\n",
      "\n",
      "---------\n",
      "\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "{ 'documents': [ Document(page_content='However, since the scaling property of “raw” agents is not the focus of these works, the scenarios/tasks and experiments considered are limited. So far, there lacks a dedicated in-depth study on this phenomenon. Hence, a natural question arises: Does this phenomenon generally exist? To answer the research question above, we conduct the first comprehensive study on the scaling property of LLM', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '1', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
      "                 Document(page_content='for llm agents: A social psychology view. arXiv preprint arXiv:2310.02124, 2023. Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., Liu, P., Nie, J.-Y., and Wen, J.-R. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. 11', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '11', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
      "                 Document(page_content='to using one single agent. Similarly, CoT-SC (Wang et al., 2023b) generates multiple thought chains and picks the most self-consistent one as the final an- swer. The reasoning performance is improved by involving more thought chains compared to chain-of-thought (CoT) (Wei et al., 2022) which employs a single thought chain. In- cidentally, from the data analysis of these works, we can no- tice the', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '1', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
      "                 Document(page_content='of experiments, we adjust these dimensions and observe their effects independently. We observe and summarize a few properties, based on which, we further develop optimization strategies that can intrigue the power of “More Agents”. Our contributions are summarized as follows: We present the first systematic study on the scaling property of raw agents instantiated by LLMs. We find that the', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '2', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
      "                 Document(page_content='multitask learners. OpenAI blog, 1(8):9, 2019. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., Chowdhery, A., and Zhou, D. Self- consistency improves chain of thought', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '10', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'})],\n",
      "  'question': '\"Mechanism of memory function in agents\"'}\n",
      "\n",
      "---------\n",
      "\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO THE QUESTION, TRY TO TRANSFORM QUERY---\n",
      "---ASSESS NUMBER OF REGENERATIONS---\n",
      "DECISION: the number of regeneration is still low. We can continue.\n",
      "\"Node 'grade_documents':\"\n",
      "{ 'documents': [],\n",
      "  'generation': 'N/A',\n",
      "  'question': '\"Mechanism of memory function in agents\"'}\n",
      "\n",
      "---------\n",
      "\n",
      "---TRANSFORM QUERY---\n",
      "\"Node 'transform_query':\"\n",
      "{ 'documents': [],\n",
      "  'generation': 'N/A',\n",
      "  'numret': 3,\n",
      "  'question': '\"Explain the mechanism responsible for memory function in '\n",
      "              'agents.\"'}\n",
      "\n",
      "---------\n",
      "\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "{ 'documents': [ Document(page_content='we isolate and analyze three dimensions of task difficulty: the inherent difficulty, the length of reasoning steps, and the prior probability of a correct answer. We find that: 1) the performance gains increase then decrease by rising the inherent difficulty; 2) performance gains increase with the number of steps; and 3) performance increases with the prior probability. Based on these properties,', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '8', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
      "                 Document(page_content='to determine the most consistent answer as the final response. Multiple LLM Agents Collaboration. We select LLM- Debate (Du et al., 2023) denoted as Debate, and self- reflection (Shinn et al., 2023) denoted as Reflection. Within these methods, we generate multiple samples by iteratively operating these methods and using majority voting to produce the final answer. Specifically, the effectiveness', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '4', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
      "                 Document(page_content='generate answers. Sub- sequently, majority voting is applied to these answers to determine the final answer. Specifically, an LLM agent refers to a single LLM or a multiple LLM-Agents collaboration framework. V (si) ← V (si) + sim(si, sj) end if end for 4. Experimental Setup We separate the experimental setup (this section) with eval- uations (next section), to introduce the coverage of scenar-', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '3', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
      "                 Document(page_content='to using one single agent. Similarly, CoT-SC (Wang et al., 2023b) generates multiple thought chains and picks the most self-consistent one as the final an- swer. The reasoning performance is improved by involving more thought chains compared to chain-of-thought (CoT) (Wei et al., 2022) which employs a single thought chain. In- cidentally, from the data analysis of these works, we can no- tice the', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '1', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'}),\n",
      "                 Document(page_content='gains range from 2% to 7%. However, two notable exceptions are observed when integrated with the debate method with the Llama2-13B and Llama2-70B models, which result in failed cases. This failure in per- formance is attributed primarily to the noise generated by referencing the answers of other agents during the debate process. The synthesized responses, which incorporate in- put from multiple', metadata={'Last Modified': '2024-05-02T21:13:10', 'Page': '4', 'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf'})],\n",
      "  'question': '\"Explain the mechanism responsible for memory function in '\n",
      "              'agents.\"'}\n",
      "\n",
      "---------\n",
      "\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO THE QUESTION, TRY TO TRANSFORM QUERY---\n",
      "---ASSESS NUMBER OF REGENERATIONS---\n",
      "DECISION: We overcome the limit of query regeneration. Exiting the loop.\n",
      "\"Node 'grade_documents':\"\n",
      "{ 'documents': [],\n",
      "  'generation': 'N/A',\n",
      "  'question': '\"Explain the mechanism responsible for memory function in '\n",
      "              'agents.\"'}\n",
      "\n",
      "---------\n",
      "\n",
      "'N/A'\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"How does memory work for agents?\", \"numret\": 1}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        pprint(value, indent=2, width=80, depth=None)\n",
    "    print(\"\\n---------\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
