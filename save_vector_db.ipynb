{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow these: \n",
    "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb\n",
    "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag_local.ipynb\n",
    "\n",
    "TODO: change embeddings; adapt json output for groq -> json_mode: https://api.python.langchain.com/en/latest/chat_models/langchain_groq.chat_models.ChatGroq.html#langchain_groq.chat_models.ChatGroq.with_structured_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `python 3.11.9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your working directory\n",
    "working_dir = \"/Users/pietromascheroni/open-modular-rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from torch import cuda\n",
    "from typing import Callable\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import chromadb\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_metadata(metadata_str: str):\n",
    "    \"\"\" transforms relevant data from a data frame column into a dict format\n",
    "    Args:\n",
    "        metadata_str (_type_): column of a dataframe\n",
    "\n",
    "    Returns:\n",
    "        _type_: column in a dict format needed for the metadata chroma function\n",
    "    \"\"\"\n",
    "    metadata_dict = {}\n",
    "    if pd.notna(metadata_str):\n",
    "        # Assuming metadata is a string formatted as \"key: value, key: value\"\n",
    "        for part in metadata_str.split(\", \"):\n",
    "            if \": \" in part:\n",
    "                key, value = part.split(\": \", 1)\n",
    "                metadata_dict[key.strip()] = value.strip()\n",
    "    return metadata_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk_ID</th>\n",
       "      <th>Content</th>\n",
       "      <th>Metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>more_agents_arxiv_paper 1 - Chunk 1</td>\n",
       "      <td>4 2 0 2 b e F 3 ] L C . s c [ 1 v 0 2 1 5 0 . ...</td>\n",
       "      <td>Source: /Users/pietromascheroni/open-modular-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>more_agents_arxiv_paper 1 - Chunk 2</td>\n",
       "      <td>LLMs, while the degree of enhancement is cor- ...</td>\n",
       "      <td>Source: /Users/pietromascheroni/open-modular-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>more_agents_arxiv_paper 1 - Chunk 3</td>\n",
       "      <td>in variety of applications (Zhao et al., 2023)...</td>\n",
       "      <td>Source: /Users/pietromascheroni/open-modular-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more_agents_arxiv_paper 1 - Chunk 4</td>\n",
       "      <td>Wu et al., 2023). In these works, multiple LLM...</td>\n",
       "      <td>Source: /Users/pietromascheroni/open-modular-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>more_agents_arxiv_paper 1 - Chunk 5</td>\n",
       "      <td>to using one single agent. Similarly, CoT-SC (...</td>\n",
       "      <td>Source: /Users/pietromascheroni/open-modular-r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Chunk_ID  \\\n",
       "0  more_agents_arxiv_paper 1 - Chunk 1   \n",
       "1  more_agents_arxiv_paper 1 - Chunk 2   \n",
       "2  more_agents_arxiv_paper 1 - Chunk 3   \n",
       "3  more_agents_arxiv_paper 1 - Chunk 4   \n",
       "4  more_agents_arxiv_paper 1 - Chunk 5   \n",
       "\n",
       "                                             Content  \\\n",
       "0  4 2 0 2 b e F 3 ] L C . s c [ 1 v 0 2 1 5 0 . ...   \n",
       "1  LLMs, while the degree of enhancement is cor- ...   \n",
       "2  in variety of applications (Zhao et al., 2023)...   \n",
       "3  Wu et al., 2023). In these works, multiple LLM...   \n",
       "4  to using one single agent. Similarly, CoT-SC (...   \n",
       "\n",
       "                                            Metadata  \n",
       "0  Source: /Users/pietromascheroni/open-modular-r...  \n",
       "1  Source: /Users/pietromascheroni/open-modular-r...  \n",
       "2  Source: /Users/pietromascheroni/open-modular-r...  \n",
       "3  Source: /Users/pietromascheroni/open-modular-r...  \n",
       "4  Source: /Users/pietromascheroni/open-modular-r...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store preprocessed and chunked data to a defined directory\n",
    "combined_df = pd.read_parquet(working_dir + '/moreAgentsPaper.parquet', engine='fastparquet')\n",
    "combined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf',\n",
       "  'Page': '1',\n",
       "  'Last Modified': '2024-05-02T21:13:10'},\n",
       " {'Source': '/Users/pietromascheroni/open-modular-rag/docs/2402.05120v1.pdf',\n",
       "  'Page': '1',\n",
       "  'Last Modified': '2024-05-02T21:13:10'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df[\"Metadata\"] = combined_df[\"Metadata\"].apply(parse_metadata)\n",
    "combined_df.Metadata.to_list()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract elements from dataframe and put them in a format suitable for chromadb\n",
    "metadatas = combined_df['Metadata'].tolist()\n",
    "ids = combined_df[['Chunk_ID']].apply(lambda x: ' '.join(x.dropna().values.tolist()), axis=1).tolist()\n",
    "documents_all = combined_df[['Content']].apply(lambda x: ' '.join(x.dropna().values.tolist()), axis=1).tolist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize embedding model and embed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pietromascheroni/open-modular-rag/env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/pietromascheroni/open-modular-rag/env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embed_model_id = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size': 32},\n",
    "    cache_folder=working_dir + '/emb_model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 139 doc embeddings, each with a dimensionality of 768.\n"
     ]
    }
   ],
   "source": [
    "# Perform Embedding\n",
    "embeddings = embedding_model.embed_documents(documents_all)\n",
    "\n",
    "print(f\"We have {len(embeddings)} doc embeddings, each with \"\n",
    "      f\"a dimensionality of {len(embeddings[0])}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromaDB setup to initilize collection including indeces of all documents\n",
    "# (in case of errors, perform pip uninstall chromadb and pip install chromadb)\n",
    "chroma_client = chromadb.PersistentClient(path=working_dir + \"/vectordb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide a name to setup and reference the vector index\n",
    "collection_name = \"more_agents_paper_self_rag\"\n",
    "# initialize the vector index with the respective similarity search metric\n",
    "vectorstore = chroma_client.get_or_create_collection(collection_name, metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the vector index with the preparred data\n",
    "vectorstore.upsert(\n",
    "    embeddings=embeddings,\n",
    "    documents=documents_all,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 139 chunks in the vector store\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {vectorstore.count()} chunks in the vector store\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
