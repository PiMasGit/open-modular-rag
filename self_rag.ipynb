{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow these: \n",
    "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb\n",
    "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag_local.ipynb\n",
    "\n",
    "TODO: change embeddings; adapt json output for groq -> json_mode: https://api.python.langchain.com/en/latest/chat_models/langchain_groq.chat_models.ChatGroq.html#langchain_groq.chat_models.ChatGroq.with_structured_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your working directory\n",
    "working_dir = \"/Users/pietro/open-modular-rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from unstructured.cleaners.core import group_broken_paragraphs\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGroq(temperature=0, groq_api_key=GROQ_API_KEY, model_name=\"llama3-70b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Justin Bieber was born on March 1, 1994. The NFL team that won the Super Bowl that year was the Dallas Cowboys, who defeated the Buffalo Bills in Super Bowl XXVIII on January 30, 1994.\n"
     ]
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "response = chain.invoke({\"text\": \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pdf_pages_with_metadata(file_path, ID_name):\n",
    "    \"\"\"PDF file loader with metadata\n",
    "    Args:\n",
    "        file_path (_type_): Path to the PDF file relative to the current working directory\n",
    "        ID_name (_type_): Identifier based on the file name\n",
    "    Returns:\n",
    "        _type_: Dataframe with the extracted data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # # Load data using the Unstructured schema with 'paged' mode -> not working on my system\n",
    "        # loader = UnstructuredFileLoader(\n",
    "        #     file_path,\n",
    "        #     mode=\"paged\",  # Use 'paged' mode to split pages correctly\n",
    "        #     strategy=\"fast\",\n",
    "        # )\n",
    "        # Used PDFMiner instead\n",
    "        loader = PDFMinerLoader(file_path, concatenate_pages=False)\n",
    "        data = loader.load()\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "        for page_number, item in enumerate(data):\n",
    "            page_content = \" \".join(item.page_content.split())\n",
    "            source = file_path\n",
    "            last_modified = item.metadata.get(\"last_modified\", \"28Apr2024\") # Modified from \"N/A\")\n",
    "\n",
    "            # Create a unique ID for each page based on ID_name and page number\n",
    "            page_id = f\"{ID_name} {page_number + 1}\"\n",
    "\n",
    "            # Append the extracted data to a list with a unique ID for each page\n",
    "            data_list.append({\n",
    "                \"ID\": page_id,  # Unique identifier for each page\n",
    "                \"Content\": page_content,\n",
    "                \"Metadata\": f\"Source: {source}, Page: {page_number + 1}, Last Modified: {last_modified}\",\n",
    "                \"DocumentType\": f\"Content: {True}\",\n",
    "            })\n",
    "\n",
    "        # Create a DataFrame from the extracted data\n",
    "        df = pd.DataFrame(data_list)\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_document_content(content: str):\n",
    "    \"\"\"Clean document content from dedicated words, phrases or patterns\n",
    "    Args:\n",
    "        content (_type_): raw text content df[\"Content\"]\n",
    "    Returns:\n",
    "        _type_: df[\"Content\"] - cleaned\n",
    "    \"\"\"\n",
    "    # Words or phrases to remove\n",
    "    words_to_remove = [\"More Agents Is All You Need\"]\n",
    "\n",
    "    # Create a regular expression pattern to match whole words\n",
    "    pattern = r'\\b(?:' + '|'.join(re.escape(word) for word in words_to_remove) + r')\\b'\n",
    "\n",
    "    # Remove specified words/phrases\n",
    "    content = re.sub(pattern, '', content, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove duplicate consecutive words\n",
    "    content = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', content)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    content = re.sub(r'\\s+', ' ', content).strip()\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def preprocess_text(text: str):\n",
    "    \"\"\"Enhanced text preprocessing.\n",
    "    - Lowercase conversion.\n",
    "    - Punctuation removal.\n",
    "    - Whitespace normalization.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def create_chunks_with_unique_ids(df, text_splitter):\n",
    "    \"\"\"function to split preprocessed document content into defined chunk sizes\n",
    "    Args:\n",
    "        df (_type_): input the dataframe with the content column\n",
    "        text_splitter (_type_): uses the RecursiveCharacterTextSplitter to split the content into chunks of size specified in chunk above\n",
    "    Returns:\n",
    "        _type_: pandas dataframe with the chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        content = row['Content']\n",
    "\n",
    "        # Split the content into chunks using the RecursiveCharacterTextSplitter\n",
    "        content_chunks = text_splitter.split_text(content)\n",
    "\n",
    "        # Generate a unique chunk ID for each chunk based on the document's ID and index\n",
    "        doc_id = row['ID']\n",
    "        for chunk_index, chunk_content in enumerate(content_chunks, start=1):\n",
    "            chunk_id = f\"{doc_id} - Chunk {chunk_index}\"\n",
    "            chunks.append({\n",
    "                'Chunk_ID': chunk_id,\n",
    "                'Content': chunk_content,\n",
    "                'Metadata': row['Metadata']\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = working_dir + \"/docs/2402.05120v1.pdf\"\n",
    "\n",
    "# load the pdf document and split the content into pages with respective meta data fields (e.g. date and page nr.)\n",
    "article_df = split_pdf_pages_with_metadata(doc_path, \"more_agents_arxiv_paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Content</th>\n",
       "      <th>Metadata</th>\n",
       "      <th>DocumentType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>more_agents_arxiv_paper 1</td>\n",
       "      <td>Junyou Li * 1 Qin Zhang * 1 Yangbin Yu 1 Qiang...</td>\n",
       "      <td>Source: /Users/pietro/open-modular-rag/docs/24...</td>\n",
       "      <td>Content: True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>more_agents_arxiv_paper 2</td>\n",
       "      <td>paths. In fact, it can be used as a plug-in to...</td>\n",
       "      <td>Source: /Users/pietro/open-modular-rag/docs/24...</td>\n",
       "      <td>Content: True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>more_agents_arxiv_paper 3</td>\n",
       "      <td>Algorithm 1 Sampling-and-voting Require: Query...</td>\n",
       "      <td>Source: /Users/pietro/open-modular-rag/docs/24...</td>\n",
       "      <td>Content: True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more_agents_arxiv_paper 4</td>\n",
       "      <td>Table 1. Comparing the conducted experiments w...</td>\n",
       "      <td>Source: /Users/pietro/open-modular-rag/docs/24...</td>\n",
       "      <td>Content: True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>more_agents_arxiv_paper 5</td>\n",
       "      <td>Figure 3. The accuracy scales with the ensembl...</td>\n",
       "      <td>Source: /Users/pietro/open-modular-rag/docs/24...</td>\n",
       "      <td>Content: True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          ID  \\\n",
       "0  more_agents_arxiv_paper 1   \n",
       "1  more_agents_arxiv_paper 2   \n",
       "2  more_agents_arxiv_paper 3   \n",
       "3  more_agents_arxiv_paper 4   \n",
       "4  more_agents_arxiv_paper 5   \n",
       "\n",
       "                                             Content  \\\n",
       "0  Junyou Li * 1 Qin Zhang * 1 Yangbin Yu 1 Qiang...   \n",
       "1  paths. In fact, it can be used as a plug-in to...   \n",
       "2  Algorithm 1 Sampling-and-voting Require: Query...   \n",
       "3  Table 1. Comparing the conducted experiments w...   \n",
       "4  Figure 3. The accuracy scales with the ensembl...   \n",
       "\n",
       "                                            Metadata   DocumentType  \n",
       "0  Source: /Users/pietro/open-modular-rag/docs/24...  Content: True  \n",
       "1  Source: /Users/pietro/open-modular-rag/docs/24...  Content: True  \n",
       "2  Source: /Users/pietro/open-modular-rag/docs/24...  Content: True  \n",
       "3  Source: /Users/pietro/open-modular-rag/docs/24...  Content: True  \n",
       "4  Source: /Users/pietro/open-modular-rag/docs/24...  Content: True  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform some cleaning operations on the extracted content\n",
    "article_df['Content'] = article_df['Content'].apply(group_broken_paragraphs)\n",
    "article_df['Content'] = article_df['Content'].apply(clean_document_content)\n",
    "article_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define how the content should be split into smaller chunks\n",
    "text_splitter_400 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk_ID</th>\n",
       "      <th>Content</th>\n",
       "      <th>Metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>more_agents_arxiv_paper 1 - Chunk 1</td>\n",
       "      <td>Junyou Li * 1 Qin Zhang * 1 Yangbin Yu 1 Qiang...</td>\n",
       "      <td>Source: /Users/pietro/open-modular-rag/docs/24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>more_agents_arxiv_paper 1 - Chunk 2</td>\n",
       "      <td>LLMs, while the degree of enhancement is cor- ...</td>\n",
       "      <td>Source: /Users/pietro/open-modular-rag/docs/24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>more_agents_arxiv_paper 1 - Chunk 3</td>\n",
       "      <td>in variety of applications (Zhao et al., 2023)...</td>\n",
       "      <td>Source: /Users/pietro/open-modular-rag/docs/24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more_agents_arxiv_paper 1 - Chunk 4</td>\n",
       "      <td>Wu et al., 2023). In these works, multiple LLM...</td>\n",
       "      <td>Source: /Users/pietro/open-modular-rag/docs/24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>more_agents_arxiv_paper 1 - Chunk 5</td>\n",
       "      <td>to using one single agent. Similarly, CoT-SC (...</td>\n",
       "      <td>Source: /Users/pietro/open-modular-rag/docs/24...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Chunk_ID  \\\n",
       "0  more_agents_arxiv_paper 1 - Chunk 1   \n",
       "1  more_agents_arxiv_paper 1 - Chunk 2   \n",
       "2  more_agents_arxiv_paper 1 - Chunk 3   \n",
       "3  more_agents_arxiv_paper 1 - Chunk 4   \n",
       "4  more_agents_arxiv_paper 1 - Chunk 5   \n",
       "\n",
       "                                             Content  \\\n",
       "0  Junyou Li * 1 Qin Zhang * 1 Yangbin Yu 1 Qiang...   \n",
       "1  LLMs, while the degree of enhancement is cor- ...   \n",
       "2  in variety of applications (Zhao et al., 2023)...   \n",
       "3  Wu et al., 2023). In these works, multiple LLM...   \n",
       "4  to using one single agent. Similarly, CoT-SC (...   \n",
       "\n",
       "                                            Metadata  \n",
       "0  Source: /Users/pietro/open-modular-rag/docs/24...  \n",
       "1  Source: /Users/pietro/open-modular-rag/docs/24...  \n",
       "2  Source: /Users/pietro/open-modular-rag/docs/24...  \n",
       "3  Source: /Users/pietro/open-modular-rag/docs/24...  \n",
       "4  Source: /Users/pietro/open-modular-rag/docs/24...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create chunks using the text splitter\n",
    "article_df = create_chunks_with_unique_ids(article_df, text_splitter_400)\n",
    "article_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store preprocessed and chunked data to a defined directory\n",
    "article_df.to_parquet(working_dir + '/moreAgentsPaper.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
